{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5792f88-2dac-474c-9121-10b872675144",
   "metadata": {},
   "source": [
    "<b><b><u>**Analysing Movie Reviews Sentiment</u></b></b>\n",
    "</br></br>We will cover aspects pertaining to natural language processing (NLP) , text analytics, and\n",
    "Machine Learning in this chapter. The problem at hand is sentiment analysis or opinion mining, where we want to analyze some textual documents and predict their sentiment or opinion based on the content of these documents. Sentiment analysis is perhaps one of the most popular applications of natural language processing and text analytics with a vast number of websites, books and tutorials on this subject. Typically sentiment analysis seems to work best on subjective text, where people express\n",
    "opinions, feelings, and their mood. From a real-world industry standpoint, sentiment analysis is widely used to analyze corporate surveys, feedback surveys, social media data, and reviews for movies, places, commodities, and many more. The idea is to analyze and understand the reactions of people toward a specific entity and take insightful actions based on their sentiment.\n",
    "-->A text corpus consists of multiple text documents and each document can be as simple as a single sentence to a complete document with multiple paragraphs. Textual data, in spite of being highly unstructured, can be\n",
    "classified into two major types of documents. Factual documents thattypically depict some form of statements or facts with no specific feelings or emotion attached to them. These are also known as objective documents.\n",
    "Subjective documents on the other hand have text that expresses feelings, moods, emotions, and opinions.\n",
    "</br></br>Sentiment analysis is also popularly known as opinion analysis or opinion mining. The key idea is to use techniques from text analytics, NLP, Machine Learning, and linguistics to extract important information or data points from unstructured text. This in turn can help us derive qualitative outputs like the overall sentiment being on a positive, neutral, or negative scale and quantitative outputs like the sentiment polarity, subjectivity, and objectivity proportions. Sentiment polarity is typically a numeric score that’s assigned to both the positive and negative aspects of a text document based on subjective parameters like specific words and phrases expressing feelings and emotion. Neutral sentiment typically has 0 polarity since it\n",
    "does not express and specific sentiment, positive sentiment will have polarity > 0, and negative < 0. Of course, you can always change these thresholds based on the type of text you are dealing with; there are no hard\n",
    "constraints on this.\n",
    "</br></br>In this chapter, we focus on trying to analyze a large corpus of movie reviews and derive the sentiment. We cover a wide variety of techniques for analyzing sentiment, which include the following.\n",
    "</br></br>1. Unsupervised lexicon-based models\n",
    "</br>2. Traditional supervised Machine Learning models\n",
    "</br>3. Newer supervised Deep Learning models\n",
    "</br>4. Advanced supervised Deep Learning models\n",
    "\n",
    "<b><b><u>Problem Statement</u></b></b> : \n",
    "The main objective in this chapter is to predict the sentiment for a number of movie reviews obtained from the Internet Movie Database (IMDb) . This dataset contains 50,000 movie reviews that have been pre-labeled with “positive” and “negative” sentiment class labels based on the review content.\n",
    "\n",
    "<b><b><u>Different packages required are</u></b></b> : numpy, scikit-learn, keras, tensorflow, theano, spacy, nltk, gensium\n",
    "</br>-->And also, For nltk you need to type the following code from a Python or ipython shell after installing nltk using either pip or conda.\n",
    "    </br>import nltk\n",
    "    </br>nltk.download('all', halt_on_error=False)\n",
    "  \n",
    "-->We also use our custom developed text pre-processing and normalization module, which you will find in the files named\n",
    "<b>contractions.py</b> and <b>text_normalizer.py</b>. Utilities related to supervised model fitting, prediction, and evaluation are present in <b>model_evaluation_utils.py</b>, so make sure you have these modules in the same directory and the other Python files and jupyter notebooks for this chapter.\n",
    "</br></br><b><b><u>Getting Data</u></b></b>\n",
    "</br>You can also download thesame data from http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00caf3e9-c338-44bb-aedc-94d15d349d08",
   "metadata": {},
   "source": [
    "<b><b><u>Sentiment Analysis Unsupervised Lexical</u></b></b>\n",
    "</br>We have installed the afinn library for the usage of the Afinn lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8070bb57-09e1-45c3-b652-2b9bc7055b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 19:28:54.896596: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 19:28:55.191306: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-20 19:28:55.241029: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-20 19:28:55.241044: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-20 19:28:56.069207: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-20 19:28:56.069271: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-20 19:28:56.069276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2249f-65eb-440d-b002-6f7876fdc9cb",
   "metadata": {},
   "source": [
    "Load and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13851bd9-72c2-41e7-8d3d-0248c091064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neel/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'movie_reviews.csv')\n",
    "\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# Extract data for model evaluation\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "sample_review_ids = [7626, 3533, 13010]\n",
    "\n",
    "# Normalize dataset\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c5910-56b2-44a3-bd3e-893065431ee9",
   "metadata": {},
   "source": [
    "Sentiment Analysis with AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a22ac1-71b9-43ad-9d59-7010860bcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afn = Afinn(emoticons=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "094fa724-982e-417f-a9c9-0c129a5d04bc",
   "metadata": {},
   "source": [
    "Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f9b65a-a220-4593-bf83-607f3e58900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW :  no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment :  negative\n",
      "Predicted Sentiment Polarity :  -7.0\n",
      "------------------------------------------------------------\n",
      "REVIEW :  I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment :  positive\n",
      "Predicted Sentiment Polarity :  3.0\n",
      "------------------------------------------------------------\n",
      "REVIEW :  Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment :  positive\n",
      "Predicted Sentiment Polarity :  -3.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print(\"REVIEW : \", review)\n",
    "    print(\"Actual Sentiment : \", sentiment)\n",
    "    print(\"Predicted Sentiment Polarity : \", afn.score(review))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab3c20-d86c-401f-b866-857d1458816e",
   "metadata": {},
   "source": [
    "Predict Sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff95671-7694-4c6b-9612-f876fc6ec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
    "predicted_sentiments = ['positive' if score >= 1.0 else 'negative' for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1470c0-4678-407b-b322-c6be92700232",
   "metadata": {},
   "source": [
    "Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba9cd031-cedc-4496-8ef0-4b3e92a7e1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7118\n",
      "Precision: 0.7289\n",
      "Recall: 0.7118\n",
      "F1 Score: 0.7062\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.85      0.75      7510\n",
      "    negative       0.79      0.57      0.67      7490\n",
      "\n",
      "    accuracy                           0.71     15000\n",
      "   macro avg       0.73      0.71      0.71     15000\n",
      "weighted avg       0.73      0.71      0.71     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3269/3196639648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_model_performance_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_sentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/neel/Projects/MachineLearningmodels/Movie Recommendation System/standford Dataset (Book)/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPrediction Confusion Matrix:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n\u001b[0m\u001b[1;32m     87\u001b[0m                              classes=classes)\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/neel/Projects/MachineLearningmodels/Movie Recommendation System/standford Dataset (Book)/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                   labels=classes)\n\u001b[1;32m     60\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 61\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     62\u001b[0m                                                   labels=level_labels), \n\u001b[1;32m     63\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416d123-0090-47de-950d-7466f61e592b",
   "metadata": {},
   "source": [
    "<b><b><u>Sentiment Analysis with SentiWordNet</u></b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f86a6c5-3deb-4d4b-8f79-5341787c8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Polarity Score :  0.875\n",
      "Negative Polarity Score :  0.125\n",
      "Objective Score :  0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\n",
    "print('Positive Polarity Score : ', awesome.pos_score())\n",
    "print('Negative Polarity Score : ', awesome.neg_score())\n",
    "print('Objective Score : ', awesome.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a7d04-0ed4-47f2-9e5b-31d0bfd81bc7",
   "metadata": {},
   "source": [
    "<b>Building Model</b>\n",
    "</br>Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text  corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context, Where\n",
    "</br>NN means none\n",
    "</br>VB means verb, And etc are as that only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28f75730-68c9-443e-8da2-db9d3cfe6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_sentiwordnet_lexicon(review,\n",
    "                                           verbose=False):\n",
    "\n",
    "    # tokenize and POS tag text tokens\n",
    "    tagged_text = [(token.text, token.tag_) for token in tn.nlp(review)]\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    # get wordnet synsets based on POS tags\n",
    "    # get sentiment scores if synsets are found\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
    "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
    "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
    "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
    "        # if senti-synset is found        \n",
    "        if ss_set:\n",
    "            # add scores for all found synsets\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    # aggregate final scores\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        # to display results in a nice table\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, norm_neg_score, norm_final_score]], columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "        ['Predicted Sentiment', 'Objectivity','Positive', 'Negative', 'Overall']], codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "        \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6573b-40dd-486f-8e6a-8aaa1a37e885",
   "metadata": {},
   "source": [
    "<b>Predict Sentiment for sample reviews</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc104d98-b697-4b1c-9c0e-9606379a62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            negative        0.68      0.1     0.22   -0.12\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.74      0.2     0.06    0.14\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive         0.8     0.12     0.07    0.05\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)    \n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202ccfc-245f-4579-b841-873b982325fc",
   "metadata": {},
   "source": [
    "<b>Predict sentiments for test dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "486c74e2-9576-4a02-8b60-d11c655294f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c73d0d-92dd-4217-9557-92d42e3070a1",
   "metadata": {},
   "source": [
    "<b>Evaluate model performance</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d2c2a85-2100-4375-835c-eae431497393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6869\n",
      "Precision: 0.6899\n",
      "Recall: 0.6869\n",
      "F1 Score: 0.6857\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.75      0.71      7510\n",
      "    negative       0.71      0.62      0.67      7490\n",
      "\n",
      "    accuracy                           0.69     15000\n",
      "   macro avg       0.69      0.69      0.69     15000\n",
      "weighted avg       0.69      0.69      0.69     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3269/3196639648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_model_performance_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_sentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/neel/Projects/MachineLearningmodels/Movie Recommendation System/standford Dataset (Book)/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPrediction Confusion Matrix:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n\u001b[0m\u001b[1;32m     87\u001b[0m                              classes=classes)\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/neel/Projects/MachineLearningmodels/Movie Recommendation System/standford Dataset (Book)/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                   labels=classes)\n\u001b[1;32m     60\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 61\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     62\u001b[0m                                                   labels=level_labels), \n\u001b[1;32m     63\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60437c-68bc-4043-b2a0-38c2558b43c8",
   "metadata": {},
   "source": [
    "<b><b>Sentiment Analysis with VADER</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "080d3cad-eb98-42c1-b2a6-3ab517fed99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86d2dd-28da-4090-b682-ee0e71322162",
   "metadata": {},
   "source": [
    "<b>Building the model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ecdd624-ff41-4c36-a18a-49330e59ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_vader_lexicon(review, threshold=0.1, verbose=False):\n",
    "    # Pre-process text\n",
    "    review = tn.strip_html_tags(review)\n",
    "    review = tn.remove_accented_chars(review)\n",
    "    review = tn.expand_contractions(review)\n",
    "    \n",
    "    # Analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # Get aggregate scores and final sentiment\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 'positive' if agg_score >=threshold else 'negative'\n",
    "    if verbose:\n",
    "        # Display detailed sentiment statistics\n",
    "        postive = str(round(scores['pos'], 2)*100)+'%'\n",
    "        final = round(agg_score, 2)\n",
    "        negative = str(round(scores['neg'], 2)*100)+'%'\n",
    "        neutral = str(round(scores['neu'], 2)*100)+'%'\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, final, postive, negative, neutral]],  columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], ['Predicted Sentiment', 'Polarity Score','Positive', 'Negative', 'Neutral']], codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d483546b-bb5f-480f-9e2f-4b21cdd8594a",
   "metadata": {},
   "source": [
    "<b>Predict sentiment for sample reviews</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75505bf4-170a-4ca7-9422-338897f9f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            negative           -0.8     0.0%    40.0%   60.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                                     \n",
      "  Predicted Sentiment Polarity Score Positive             Negative Neutral\n",
      "0            negative          -0.16    16.0%  14.000000000000002%   69.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            positive           0.49    11.0%    11.0%   77.0%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677d47a-1359-4a84-87fe-aa267962b02a",
   "metadata": {},
   "source": [
    "<b>Predict Sentiment for test dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "338a954b-886e-47d2-9c64-ee46a928c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neel/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12644905-705e-4c85-85d9-9b64fd48c9cc",
   "metadata": {},
   "source": [
    "<b>Evaluate model performance</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "388af1c5-9d3f-4f56-a808-f706f6e0a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7109\n",
      "Precision: 0.7238\n",
      "Recall: 0.7109\n",
      "F1 Score: 0.7066\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.83      0.74      7510\n",
      "    negative       0.78      0.59      0.67      7490\n",
      "\n",
      "    accuracy                           0.71     15000\n",
      "   macro avg       0.72      0.71      0.71     15000\n",
      "weighted avg       0.72      0.71      0.71     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3269/3196639648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_model_performance_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_sentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/neel/Projects/MachineLearningmodels/Movie Recommendation System/standford Dataset (Book)/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPrediction Confusion Matrix:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n\u001b[0m\u001b[1;32m     87\u001b[0m                              classes=classes)\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/neel/Projects/MachineLearningmodels/Movie Recommendation System/standford Dataset (Book)/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_confusion_matrix\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                   labels=classes)\n\u001b[1;32m     60\u001b[0m     cm_frame = pd.DataFrame(data=cm, \n\u001b[0;32m---> 61\u001b[0;31m                             columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n\u001b[0m\u001b[1;32m     62\u001b[0m                                                   labels=level_labels), \n\u001b[1;32m     63\u001b[0m                             index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
