{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef50970e-4c2a-46a0-ab42-b0ff7ecd8599",
   "metadata": {},
   "source": [
    "<b><b>Importing Necessary Dependencies</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ac445c-8011-41d4-8084-19feb5b4821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 00:09:15.152021: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-23 00:09:15.288110: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-23 00:09:15.291186: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-23 00:09:15.291199: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-23 00:09:15.761287: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-23 00:09:15.761326: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-23 00:09:15.761330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c85412-c85c-4ee6-a11e-0f8e7c2e1486",
   "metadata": {},
   "source": [
    "<b><b>Load and normalize data</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66ab7b6-430e-4633-9c78-ca5682010580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                   review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neel/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'movie_reviews.csv')\n",
    "\n",
    "# Take a peek at the data\n",
    "print(dataset.head)\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# Build train and test datasets\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "\n",
    "# Normalize datasets\n",
    "norm_train_reviews = tn.normalize_corpus(train_reviews)\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356c904-1753-4b26-81ab-9c41b2102688",
   "metadata": {},
   "source": [
    "<b><b>Tokenize train & test dataset</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfc3a40-3623-4bf8-ab89-624f95333c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [tn.tokenizer.tokenize(text) for text in norm_train_reviews]\n",
    "tokenized_test = [tn.tokenizer.tokenize(text) for text in norm_test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afc224-26dc-4b3a-96e8-715c49cceabd",
   "metadata": {},
   "source": [
    "<b><b>Build Vocabulary Mapping (word to index)</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08db514-d63f-41b1-902b-6713789e989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 82095\n",
      "Sample slice of vocabulary map: {'happen': 11, 'I': 12, 'first': 13, 'thing': 14, 'strike': 15, 'brutality': 16, 'unflinche': 17, 'scene': 18, 'violence': 19, 'set': 20}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build word to index vocabulary\n",
    "token_counter = Counter([token for review in tokenized_train for token in review])\n",
    "vocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\n",
    "max_index = np.max(list(vocab_map.values()))\n",
    "vocab_map['PAD_INDEX']=0\n",
    "vocab_map['NOT_FOUND_INDEX']= max_index+1\n",
    "vocab_size = len(vocab_map)\n",
    "# View vocabulary size and part of the vocabulary map\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample slice of vocabulary map:', dict(list(vocab_map.items())[10:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7ef0a-d539-49a7-97ef-bf5b55b0f4c9",
   "metadata": {},
   "source": [
    "<b><b>Encode and pad datasets & Encode prediction class labels</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6641b02e-b368-4bb4-8d7e-8e9d24d8d93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of train review vectors: 1445\n",
      "Train review vectors shape: (35000, 1445)  Test review vectors shape: (15000, 1445)\n"
     ]
    }
   ],
   "source": [
    "from keras import preprocessing\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# get max length of train corpus and initialize label encoder\n",
    "le = LabelEncoder()\n",
    "num_classes=2 # positive -> 1, negative -> 0\n",
    "max_len = np.max([len(review) for review in tokenized_train])\n",
    "\n",
    "## Train reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "train_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\n",
    "train_X = pad_sequences(train_X, maxlen=max_len) # pad \n",
    "## Train prediction class labels\n",
    "# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\n",
    "train_y = le.fit_transform(train_sentiments)\n",
    "\n",
    "## Test reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "test_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX'] for token in tokenized_review] for tokenized_review in tokenized_test]\n",
    "test_X = pad_sequences(test_X, maxlen=max_len)\n",
    "## Test prediction class labels\n",
    "# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\n",
    "test_y = le.transform(test_sentiments)\n",
    "\n",
    "# view vector shapes\n",
    "print('Max length of train review vectors:', max_len)\n",
    "print('Train review vectors shape:', train_X.shape, ' Test review vectors shape:', test_X.shape)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b295a-bec9-4009-ab84-23fc79fe1fa9",
   "metadata": {},
   "source": [
    "<b><b>Build the LSTM Model Architecture</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b56b07a-27c6-448a-9387-e81a2057dab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 00:50:17.052538: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-23 00:50:17.052891: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-23 00:50:17.052912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2022-12-23 00:50:17.053657: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "EMBEDDING_DIM = 128 #Dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # Total LSTM units\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length = max_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90ebaabd-44bd-4890-aaa2-b0ea8ee35ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1445, 128)         10508160  \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 1445, 128)        0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,557,633\n",
      "Trainable params: 10,557,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b72ec0-5563-4f1c-8505-d43d961c55c4",
   "metadata": {},
   "source": [
    "<b><b>Visualize model architecture</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb370463-15a9-49c5-82b3-24e45b685860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1804pt\" height=\"104pt\" viewBox=\"0.00 0.00 1353.00 78.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1.33 1.33) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-74 1349,-74 1349,4 -4,4\"/>\n",
       "<!-- 139756147799472 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139756147799472</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-69.5 240,-69.5 240,-0.5 0,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"120\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\">InputLayer</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-46.5 240,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"57.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"115,-23.5 115,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-23.5 240,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"60\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 1445)]</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"120,-0.5 120,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"180\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 1445)]</text>\n",
       "</g>\n",
       "<!-- 139756147885632 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139756147885632</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"276,-0.5 276,-69.5 532,-69.5 532,-0.5 276,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"404\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\">Embedding</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"276,-46.5 532,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"337.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"399,-23.5 399,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"465.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"276,-23.5 532,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"331\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1445)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"386,-0.5 386,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1445, 128)</text>\n",
       "</g>\n",
       "<!-- 139756147799472&#45;&gt;139756147885632 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139756147799472-&gt;139756147885632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M240.18,-35C248.52,-35 256.96,-35 265.38,-35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"265.64,-38.5 275.64,-35 265.64,-31.5 265.64,-38.5\"/>\n",
       "</g>\n",
       "<!-- 139756147726752 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139756147726752</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"568,-0.5 568,-69.5 860,-69.5 860,-0.5 568,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"714\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\">SpatialDropout1D</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"568,-46.5 860,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"638.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"709,-23.5 709,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"784.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"568,-23.5 860,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"641\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1445, 128)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"714,-0.5 714,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"787\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1445, 128)</text>\n",
       "</g>\n",
       "<!-- 139756147885632&#45;&gt;139756147726752 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139756147885632-&gt;139756147726752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M532.05,-35C540.57,-35 549.2,-35 557.83,-35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"557.96,-38.5 567.96,-35 557.96,-31.5 557.96,-38.5\"/>\n",
       "</g>\n",
       "<!-- 139756150597952 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139756150597952</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"896,-0.5 896,-69.5 1134,-69.5 1134,-0.5 896,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1015\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\">LSTM</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"896,-46.5 1134,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"953\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1010,-23.5 1010,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"1072\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"896,-23.5 1134,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"969\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1445, 128)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1042,-0.5 1042,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"1088\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 139756147726752&#45;&gt;139756150597952 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139756147726752-&gt;139756150597952</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M860.12,-35C868.58,-35 877.06,-35 885.46,-35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"885.64,-38.5 895.64,-35 885.64,-31.5 885.64,-38.5\"/>\n",
       "</g>\n",
       "<!-- 139756147776384 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139756147776384</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1170,-0.5 1170,-69.5 1345,-69.5 1345,-0.5 1170,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1257.5\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1170,-46.5 1345,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"1211.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1253,-23.5 1253,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"1299\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1170,-23.5 1345,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"1216\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 64)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1262,-0.5 1262,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"1303.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 139756150597952&#45;&gt;139756147776384 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139756150597952-&gt;139756147776384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1134.14,-35C1142.67,-35 1151.21,-35 1159.6,-35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1159.74,-38.5 1169.74,-35 1159.74,-31.5 1159.74,-38.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, rankdir='LR').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4267c0b-c1b2-4c7f-9e62-53953d49ba90",
   "metadata": {},
   "source": [
    "<b><b>Train the model</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e2428ba-511b-413f-9308-d10a5c3e6c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "315/315 [==============================] - 351s 1s/step - loss: 0.3620 - accuracy: 0.8353 - val_loss: 0.2694 - val_accuracy: 0.8946\n",
      "Epoch 2/5\n",
      "315/315 [==============================] - 347s 1s/step - loss: 0.1763 - accuracy: 0.9349 - val_loss: 0.2939 - val_accuracy: 0.8826\n",
      "Epoch 3/5\n",
      "315/315 [==============================] - 331s 1s/step - loss: 0.1014 - accuracy: 0.9658 - val_loss: 0.3556 - val_accuracy: 0.8760\n",
      "Epoch 4/5\n",
      "315/315 [==============================] - 334s 1s/step - loss: 0.0832 - accuracy: 0.9716 - val_loss: 0.4060 - val_accuracy: 0.8777\n",
      "Epoch 5/5\n",
      "315/315 [==============================] - 334s 1s/step - loss: 0.0406 - accuracy: 0.9874 - val_loss: 0.5662 - val_accuracy: 0.8694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b70295f40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "model.fit(train_X, train_y, epochs=5, batch_size=batch_size, shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f74ba7-0901-4712-8b91-37863ac0c271",
   "metadata": {},
   "source": [
    "<b><b>Predict and Evaluate Model Performance</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8910665-f784-410d-aa3c-8431010c1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5237/400084823.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  pred_test = model.predict_generator(test_X)\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict_generator(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f7e03b4-4c6d-48a4-a7ac-9f0cfc4e421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.44e-05]\n",
      " [1.00e+00]\n",
      " [4.83e-06]\n",
      " ...\n",
      " [4.76e-03]\n",
      " [6.76e-01]\n",
      " [3.60e-05]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5237/3716659133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# predictions=[]\n",
    "print(pred_test)\n",
    "predictions=[]\n",
    "for temp in pred_test:\n",
    "    if temp[0] < temp[1]:\n",
    "        predictions.append('positive')\n",
    "    else:\n",
    "        predictions.appedn('negative')\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9aaf87c-034c-4a95-a1ca-70ec1f6189ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15000, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5237/2092252869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_model_performance_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/neel/Projects/MachineLearningmodels/Movie Recommendation System/standford Dataset (Book)/model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[0;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Performance metrics:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nModel Classification report:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/neel/Projects/MachineLearningmodels/Movie Recommendation System/standford Dataset (Book)/model_evaluation_utils.py\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(true_labels, predicted_labels)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     print('Accuracy:', np.round(\n\u001b[0;32m---> 23\u001b[0;31m                         metrics.accuracy_score(true_labels, \n\u001b[0m\u001b[1;32m     24\u001b[0m                                                predicted_labels),\n\u001b[1;32m     25\u001b[0m                         4))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15000, 0]"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96611c0-885f-4c02-9c25-d5776deeb84c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
